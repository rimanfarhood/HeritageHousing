{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Data Cleaning Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Evaluate missing data\n",
        "*   Clean data\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/SalePrice.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate cleaned Train and Test sets, both saved under outputs/datasets/cleaned\n",
        "\n",
        "## Conclusions\n",
        "\n",
        " \n",
        "  * Data Cleaning Pipeline\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kspgyffxear-"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tdAGw4Zwssu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from ydata_profiling import ProfileReport\n",
        "from feature_engine.imputation import CategoricalImputer\n",
        "from feature_engine.imputation import MeanMedianImputer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Collected data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2ELZj83tF1g",
        "outputId": "d51e2567-7f6c-4200-b9c2-d4982d22e6fd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"outputs/datasets/collection/HousePricing.csv\")\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iue5e5GJ_vZg"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhWelcb_17dw"
      },
      "source": [
        "In Data Cleaning you are interested to check the distribution and shape of a variable with missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "vars_with_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyi3gi2-_q1j"
      },
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "if vars_with_missing_data:\n",
        "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"There are no variables with missing data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYdZHyDhu4kG"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxoVRefhu2Bk"
      },
      "source": [
        "## Assessing Missing Data Levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcocWZkIx6nk"
      },
      "source": [
        "* Custom function to display missing data levels in a DataFrame, it shows the absolute levels, relative levels and data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9CoLqBhO7ga"
      },
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "    missing_data_absolute = df.isnull().sum()\n",
        "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
        "    df_missing_data = (pd.DataFrame(\n",
        "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
        "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
        "                                   \"DataType\": df.dtypes}\n",
        "                                    )\n",
        "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
        "                          .query(\"PercentageOfDataset > 0\")\n",
        "                          )\n",
        "\n",
        "    return df_missing_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHrzaG-ZhEKt"
      },
      "source": [
        "Check missing data levels for the collected dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "zxvNnLAjxCSi",
        "outputId": "e6239998-21e1-4401-80de-e25c33bbba52"
      },
      "outputs": [],
      "source": [
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsL8yYQEyOSt"
      },
      "source": [
        "## Dealing with Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* This is a custom function from the 'Feature Engineering Lesson Notebook unit 9'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def DataCleaningEffect(df_original,df_cleaned,variables_applied_with_method):\n",
        "\n",
        "  flag_count=1 # Indicate plot number\n",
        "  \n",
        "  # distinguish between numerical and categorical variables\n",
        "  categorical_variables = df_original.select_dtypes(exclude=['number']).columns \n",
        "\n",
        "  # scan over variables, \n",
        "    # first on variables that you applied the method\n",
        "    # if the variable is a numerical plot, a histogram if categorical plot a barplot\n",
        "  for set_of_variables in [variables_applied_with_method]:\n",
        "    print(\"\\n=====================================================================================\")\n",
        "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
        "    print(f\"{set_of_variables} \\n\\n\")\n",
        "  \n",
        "\n",
        "    for var in set_of_variables:\n",
        "      if var in categorical_variables:  # it is categorical variable: barplot\n",
        "        \n",
        "        df1 = pd.DataFrame({\"Type\":\"Original\",\"Value\":df_original[var]})\n",
        "        df2 = pd.DataFrame({\"Type\":\"Cleaned\",\"Value\":df_cleaned[var]})\n",
        "        dfAux = pd.concat([df1, df2], axis=0)\n",
        "        fig , axes = plt.subplots(figsize=(15, 5))\n",
        "        sns.countplot(hue='Type', data=dfAux, x=\"Value\",palette=['#432371',\"#FAAE7B\"])\n",
        "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.legend() \n",
        "\n",
        "      else: # it is numerical variable: histogram\n",
        "\n",
        "        fig , axes = plt.subplots(figsize=(10, 5))\n",
        "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True,element=\"step\", ax=axes)\n",
        "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True,element=\"step\", ax=axes)\n",
        "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.legend() \n",
        "\n",
        "      plt.show()\n",
        "      flag_count+= 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt8Yqjy6ghyw"
      },
      "source": [
        "### Data Cleaning Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4GaYe_DgqwT"
      },
      "source": [
        "Potential Cleaning approaches \n",
        "\n",
        "* Median Imputer - `['2ndFlrSF', 'BedroomAbvGr', 'GarageYrBlt', 'LotFrontage', 'MasVnrArea']` since the distribution isn't bell shape curved\n",
        "\n",
        "* Categorical Imputer - `['BsmtFinType1','GarageFinish']`. \n",
        "\n",
        "* Simple Imputer `['EnclosedPorch', 'WoodDeckSF']` Have a high percentage of missing values, but in real life not all houses do have a porch nor a wood deck area so this will be filled with 0. \n",
        "\n",
        "\n",
        "**The list above is your guide, your map to know at which stage you are in the data-cleaning process**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W12Z8KIPoZ-8"
      },
      "source": [
        "### Split Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knk7DcVborLI",
        "outputId": "049ef5f2-c431-4a9d-fd87-00809986c38d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['SalePrice'],\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=0)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "zdMPKBU_yCpF",
        "outputId": "df45adaf-5e93-4644-919f-6b82e51fe8b7"
      },
      "outputs": [],
      "source": [
        "df_missing_data = EvaluateMissingData(TrainSet)\n",
        "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
        "df_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "median_imp =  ['2ndFlrSF', 'BedroomAbvGr', 'GarageYrBlt', 'LotFrontage', 'MasVnrArea']\n",
        "drop = ['EnclosedPorch', 'WoodDeckSF',]\n",
        "missing_cat = ['BsmtFinType1','GarageFinish']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MeanMedianImputer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Impute missing values with median** \n",
        "\n",
        "Since the missing data levels isn't so high \n",
        "- `['2ndFlrSF', 'BedroomAbvGr', 'GarageYrBlt', 'LotFrontage', 'MasVnrArea']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_method = median_imp\n",
        "imputer = MeanMedianImputer(imputation_method='median', variables=variables_method)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "\n",
        "DataCleaningEffect(df_original=df,\n",
        "                   df_cleaned=df_method,\n",
        "                   variables_applied_with_method=variables_method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Imputation method 'missing'**\n",
        "- `['BsmtFinType1','GarageFinish']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_method = missing_cat\n",
        "imputer = CategoricalImputer(imputation_method='missing', fill_value='No Record', variables=variables_method)\n",
        "df_method =imputer.fit_transform(TrainSet)\n",
        "\n",
        "DataCleaningEffect(df_original=df,\n",
        "                   df_cleaned=df_method,\n",
        "                   variables_applied_with_method=variables_method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The distribution has been impacted as thought but for now i will apply this changes to prepare the model for data similar to the real-time data, \n",
        "from my research real estate house data is usually right skewed towards the higher prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc9pg4ww_BO-"
      },
      "source": [
        "* In this case, there is no effect on the distribution of the variable, since you are not removing rows, but columns.\n",
        "* The effect might be losing features that might have a relevant impact on your machine-learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBq1a_Me4qr-"
      },
      "source": [
        "* Step 5: If you are satisfied, apply the transformation to your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "pipeline = Pipeline([\n",
        "      ('Median', MeanMedianImputer(imputation_method='median', variables=median_imp) ),\n",
        "      ('CategoricalImputer', CategoricalImputer(imputation_method='missing', fill_value='No Record', variables=missing_cat)),\n",
        "      ('Drop', DropFeatures(features_to_drop=drop))\n",
        "])\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = TrainSet.copy()\n",
        "test = TestSet.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = pipeline.transform(TrainSet) , pipeline.transform(TestSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(TrainSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD--GI5Jvp8h"
      },
      "source": [
        "# Push cleaned data to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE3DQfy82r7I"
      },
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.to_csv(\"outputs/datasets/collection/raw/TrainRaw.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1ayKT4F2p52"
      },
      "outputs": [],
      "source": [
        "TrainSet.to_csv(\"outputs/datasets/collection/cleaned/TrainSet.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTTabVk-2ulD"
      },
      "source": [
        "## Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test.to_csv(\"outputs/datasets/collection/raw/TestRaw.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCjorYny2qCD"
      },
      "outputs": [],
      "source": [
        "TestSet.to_csv(\"outputs/datasets/collection/cleaned/TestSet.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Good job! Clear cell outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgNVNpuV6NV2"
      },
      "source": [
        "Well done! You can now push the changes to your GitHub Repo, using the Git commands (git add, git commit, git push)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
